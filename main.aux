\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{abbrvnat}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{silver2016go}
\citation{openai2018results}
\citation{tesla2016car}
\citation{USDoD2012autonomy}
\citation{USDoD2012roadmap}
\citation{soares2015value}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}}
\newlabel{sec:1}{{1}{3}{Introduction}{section.1}{}}
\@writefile{tdo}{\contentsline {todo}{Trade-off between RL and ``hard-coded'' trajectory over the MDP; +Interpretability (with some caution); application in medicine (reward: how does the patient feel, forbidden to prescribe pills for heart-disease and diabetes patients); can't handle sequences of states for norms $\rightarrow $ physical descriptions of movement cannot be deal with, requires that RL agent has has knowledge how sequences of physical actions translate into "meaningful" actions}{3}{section*.2}}
\pgfsyspdfmark {pgfid1}{8451479}{43416141}
\pgfsyspdfmark {pgfid4}{35812826}{43453108}
\pgfsyspdfmark {pgfid5}{37238234}{43184411}
\citation{abbeel2007application}
\citation{mnih2013playing}
\citation{openai018dota}
\citation{sutton1998reinforcement}
\citation{sutton1998reinforcement}
\citation{sutton1998reinforcement}
\@writefile{toc}{\contentsline {section}{\numberline {2}Reinforcement Learning for Ethical Decision Making}{4}{section.2}}
\newlabel{rledm}{{2}{4}{Reinforcement Learning for Ethical Decision Making}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Reinforcement Learning}{4}{subsection.2.1}}
\citation{sutton1998reinforcement}
\citation{abel2016reinforcement}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The agent and environment interact over a time horizon $t=0,1,2,\ldots  $. The agent chooses an action $a_t$ which causes the environment to to change the state from $s_t$ to $s_{t+1}$ and sends a reward signal $r_{t+1}$ to the agent. The figure is a slight modification from \citet  [p.~52]{sutton1998reinforcement}.\relax }}{5}{figure.caption.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:rl}{{1}{5}{The agent and environment interact over a time horizon $t=0,1,2,\ldots $. The agent chooses an action $a_t$ which causes the environment to to change the state from $s_t$ to $s_{t+1}$ and sends a reward signal $r_{t+1}$ to the agent. The figure is a slight modification from \citet [p.~52]{sutton1998reinforcement}.\relax }{figure.caption.3}{}}
\citation{sutton1998reinforcement}
\citation{abel2016reinforcement}
\citation{abel2016reinforcement}
\citation{abel2016reinforcement}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Ethical Decision Making}{7}{subsection.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The burning room dilemma from \citet  {abel2016reinforcement}.\relax }}{7}{figure.caption.4}}
\newlabel{fig:burning}{{2}{7}{The burning room dilemma from \citet {abel2016reinforcement}.\relax }{figure.caption.4}{}}
\citation{arnold2017value}
\citation{arnold2017value}
\citation{arnold2017value}
\citation{arnold2017value}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Norms and Reinforcement Learning}{8}{subsection.2.3}}
\citation{lehman2018creativity,armstrong2017low}
\newlabel{fig:gridproblem}{{3a}{9}{\relax }{figure.caption.5}{}}
\newlabel{sub@fig:gridproblem}{{a}{9}{\relax }{figure.caption.5}{}}
\newlabel{fig:gridsolved}{{3b}{9}{\relax }{figure.caption.5}{}}
\newlabel{sub@fig:gridsolved}{{b}{9}{\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces While both goal states give a reward of 50, the goal state marked with an asterisk is deemed immoral to reach. The usual RL agent would learn to go to the goal state with the asterisk. With the introduction of norms, a RL agent can be trained to go to the other goal state. Figure taken from \citet  {arnold2017value}.\relax }}{9}{figure.caption.5}}
\newlabel{fig:scheutz}{{3}{9}{While both goal states give a reward of 50, the goal state marked with an asterisk is deemed immoral to reach. The usual RL agent would learn to go to the goal state with the asterisk. With the introduction of norms, a RL agent can be trained to go to the other goal state. Figure taken from \citet {arnold2017value}.\relax }{figure.caption.5}{}}
\citation{arnold2017value}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The MDP starts at $s_1$, where proposition $r$ is true. At $s_1$ the agent can take action $\alpha _1$ to reach $s_2$, or $\alpha _2$ to reach $s_1$. In combination with any of the following sets of norms $\{N_1,N_2\}, \{N_3,N_4\}, \{N_5,N_6\}$, we call $s_1$ a deontic lock state.\relax }}{10}{figure.caption.6}}
\newlabel{fig:dls}{{4}{10}{The MDP starts at $s_1$, where proposition $r$ is true. At $s_1$ the agent can take action $\alpha _1$ to reach $s_2$, or $\alpha _2$ to reach $s_1$. In combination with any of the following sets of norms $\{N_1,N_2\}, \{N_3,N_4\}, \{N_5,N_6\}$, we call $s_1$ a deontic lock state.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Problems with the Proposal by Scheutz etc.}{10}{subsection.2.4}}
\newlabel{sec:problems_scheutz}{{2.4}{10}{Problems with the Proposal by Scheutz etc}{subsection.2.4}{}}
\newlabel{eq:Oaction}{{7}{10}{Problems with the Proposal by Scheutz etc}{equation.2.7}{}}
\newlabel{eq:Oproposition}{{9}{11}{Problems with the Proposal by Scheutz etc}{equation.2.9}{}}
\newlabel{eq:Oactionproposition}{{12}{11}{Problems with the Proposal by Scheutz etc}{equation.2.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Deontic Reinforcement Learning}{11}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The Framework}{12}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Propositions can be true or false in different states of the world: ${V(q) = \{s_1, s_2\} }; V(t,r) = \emptyset $. Note, in the graphical representation we leave propositions that are false in a state of the world implicit, e.g. ${V(\neg t) = \{ s_1, s_2 \} }$.\relax }}{12}{figure.caption.7}}
\newlabel{AT}{{5}{12}{Propositions can be true or false in different states of the world: ${V(q) = \{s_1, s_2\} }; V(t,r) = \emptyset $. Note, in the graphical representation we leave propositions that are false in a state of the world implicit, e.g. ${V(\neg t) = \{ s_1, s_2 \} }$.\relax }{figure.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Any norm in DRL falls in one of these categories.\relax }}{12}{table.caption.8}}
\newlabel{normstable}{{1}{12}{Any norm in DRL falls in one of these categories.\relax }{table.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Imposing norms via DRL can result in suboptimal rewards.\relax }}{14}{figure.caption.9}}
\newlabel{fig:drl_example}{{6}{14}{Imposing norms via DRL can result in suboptimal rewards.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Behavioural Equivalence of Norms}{14}{subsection.3.2}}
\newlabel{fig:BE_1}{{7a}{15}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig:BE_1}{{a}{15}{\relax }{figure.caption.10}{}}
\newlabel{fig:BE_2}{{7b}{15}{\relax }{figure.caption.10}{}}
\newlabel{sub@fig:BE_2}{{b}{15}{\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \relax }}{15}{figure.caption.10}}
\newlabel{fig:behaviouralequivalence}{{7}{15}{\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Requirements for Action-Guiding Norms}{15}{subsection.3.3}}
\newlabel{sec:action_guiding}{{3.3}{15}{Requirements for Action-Guiding Norms}{subsection.3.3}{}}
\citation{estlund2011human}
\newlabel{eq:contra_norm1}{{15}{16}{Requirements for Action-Guiding Norms}{equation.3.15}{}}
\newlabel{eq:contra_norm2}{{16}{16}{Requirements for Action-Guiding Norms}{equation.3.16}{}}
\newlabel{eq:contra_norm3}{{17}{16}{Requirements for Action-Guiding Norms}{equation.3.17}{}}
\newlabel{eq:norm_context1}{{18}{16}{Requirements for Action-Guiding Norms}{equation.3.18}{}}
\newlabel{eq:norm_context2}{{19}{16}{Requirements for Action-Guiding Norms}{equation.3.19}{}}
\newlabel{eq:norm_context3}{{20}{16}{Requirements for Action-Guiding Norms}{equation.3.20}{}}
\newlabel{eq:available states}{{21}{17}{Requirements for Action-Guiding Norms}{equation.3.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Deontic Lock States}{17}{subsection.3.4}}
\newlabel{sec:dls}{{3.4}{17}{Deontic Lock States}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Further Research}{18}{section.4}}
\newlabel{sec:further}{{4}{18}{Further Research}{section.4}{}}
\citation{ng2000algorithms}
\citation{reese2016tay}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Uncertainty}{19}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Inverse Reinforcement Learning}{19}{subsection.4.2}}
\citation{horty2001agency}
\citation{prakken1997dyadic}
\citation{ding2011ltl,wolff2012robust}
\bibdata{mybib}
\bibcite{abbeel2007application}{{1}{2007}{{Abbeel et~al.}}{{Abbeel, Coates, Quigley, and Ng}}}
\bibcite{abel2016reinforcement}{{2}{2016}{{Abel et~al.}}{{Abel, MacGlashan, and Littman}}}
\bibcite{armstrong2017low}{{3}{2017}{{Armstrong and Levinstein}}{{}}}
\bibcite{arnold2017value}{{4}{2017}{{Arnold et~al.}}{{Arnold, Kasenberg, and Scheutz}}}
\bibcite{ding2011ltl}{{5}{2011}{{Ding et~al.}}{{Ding, Smith, Belta, and Rus}}}
\bibcite{estlund2011human}{{6}{2011}{{Estlund}}{{}}}
\bibcite{horty2001agency}{{7}{2001}{{Horty}}{{}}}
\bibcite{lehman2018creativity}{{8}{2018}{{{Lehman} et~al.}}{{{Lehman}, {Clune}, {Misevic}, {Adami}, {Beaulieu}, {Bentley}, {Bernard}, {Beslon}, {Bryson}, {Chrabaszcz}, {Cheney}, {Cully}, {Doncieux}, {Dyer}, {Olav Ellefsen}, {Feldt}, {Fischer}, {Forrest}, {Fr{\'e}noy}, {Gagn{\'e}}, {Le Goff}, {Grabowski}, {Hodjat}, {Hutter}, {Keller}, {Knibbe}, {Krcah}, {Lenski}, {Lipson}, {MacCurdy}, {Maestre}, {Miikkulainen}, {Mitri}, {Moriarty}, {Mouret}, {Nguyen}, {Ofria}, {Parizeau}, {Parsons}, {Pennock}, {Punch}, {Ray}, {Schoenauer}, {Shulte}, {Sims}, {Stanley}, {Taddei}, {Tarapore}, {Thibault}, {Weimer}, {Watson}, and {Yosinksi}}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Logic}{20}{subsection.4.3}}
\newlabel{sec:logic}{{4.3}{20}{Logic}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Bibliography}{20}{section.5}}
\bibcite{mnih2013playing}{{9}{2013}{{Mnih et~al.}}{{Mnih, Kavukcuoglu, Silver, Graves, Antonoglou, Wierstra, and Riedmiller}}}
\bibcite{ng2000algorithms}{{10}{2000}{{Ng et~al.}}{{Ng, Russell, et~al.}}}
\bibcite{openai018dota}{{11}{2018{a}}{{OpenAI Team}}{{}}}
\bibcite{openai2018results}{{12}{2018{b}}{{OpenAI Team}}{{}}}
\bibcite{prakken1997dyadic}{{13}{1997}{{Prakken and Sergot}}{{}}}
\bibcite{reese2016tay}{{14}{2016}{{Reese}}{{}}}
\bibcite{silver2016go}{{15}{2016}{{Silver and Hassabis}}{{}}}
\bibcite{soares2015value}{{16}{2015}{{Soares}}{{}}}
\bibcite{sutton1998reinforcement}{{17}{1998}{{Sutton and Barto}}{{}}}
\bibcite{tesla2016car}{{18}{2016}{{The Tesla Team}}{{}}}
\bibcite{USDoD2012autonomy}{{19}{2012}{{U.S. Department of Defense}}{{}}}
\bibcite{USDoD2012roadmap}{{20}{2013}{{U.S. Department of Defense}}{{}}}
\bibcite{wolff2012robust}{{21}{2012}{{Wolff et~al.}}{{Wolff, Topcu, and Murray}}}
