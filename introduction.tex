%!Tex root = ./Main.tex
\section{Introduction}
\label{sec:1}

%As AI becomes increasingly autonomous, it becomes an urgent task to ensure that the behaviour of AI is aligned with human values, i.e. that certain legal, moral and social norms are not violated. That AI is increasingly autonomous implies the claim that it is, unlike traditional AI, not completely pre-programmed.\footnote{Autonomy is a very contested concept. See ?? for different interpretations.}\todo{CITE, CITE} Instead, the AI learns from experience to change its programming.

\todo{Trade-off between RL and ``hard-coded'' trajectory over the MDP; +Interpretability (with some caution); application in medicine (reward: how does the patient feel, forbidden to prescribe pills for heart-disease and diabetes patients); can't handle sequences of states for norms $\rightarrow$ physical descriptions of movement cannot be deal with, requires that RL agent has has knowledge how sequences of physical actions translate into "meaningful" actions}

As AI becomes more advanced, it will be used to solve problems in complex, real-world environments. With `more advanced' I mean two different directions of AI development. First, over time AI will be capable of solving increasingly difficult problems. While ten years ago it was wishful thinking that an AI could play Go, in 2015 AlphaGo was the first program to beat a professional Go player \citep{silver2016go}. OpenAI Five was able to beat top human players at the video game DOTA at the beginning of August 2018 \citep{openai2018results}. Multiple car manufacturers are in the process of creating self-driving cars \citep{tesla2016car}. Over the last decade, the US military is increasingly interested in developing autonomous weapon systems \citep{USDoD2012autonomy}. These systems come with the promise of highly efficient robots that can be deployed in situations that are too dangerous for humans, e.g. nuclear fallout areas \citep{USDoD2012roadmap}. 

%Armies all over the world are interested in using AI for warfare \citep{Kott2018cyberdefense}.
%Tesla, among others, is developing self-driving cars , and Microsoft is working on mapping the immune system for a better predictive medicinal system \citep{microsoft2018health}.

Second, current systems are considered to be weak AI. That means that the AI is limited to a specific, narrow task. For example, a chess program is not capable of playing Go and vice versa. In contrast to weak AI stands artificial general intelligence (AGI), which is more domain independent. An AGI, as envisioned in this paper, would be capable of preparing your breakfast, helping your children do their homework, making appointments on your behalf, as well as playing chess against you, among other things.

%In recent years there has been an increasing interest in ensuring that AI is safe to use, i.e. that the AI behaves in ways that aligns with human values.

When AI becomes advanced as described, it will possess the capability to harm humans or violate our rights in various ways. Even more troublesome, the specific decision making process that lead to the right violating behaviour might be unintelligible for humans. This is often the case for artificial neural networks. While we can understand what the network as a whole is trying to accomplish, the task of a single neuron is often outside our grasp. This kind of problemis also known as an `interpretability problem' and contributed to questions such as whether humans can be held responsible for wrongful acts committed by an AI \citep{sparrow2007killer,swoboda2018responsibility}.

Hence, there are two problems that developers of AGI have to address. First, what kind of framework is adequate to ensure that AGI reliably behaves in morally permissible ways? Second, how (if at all) can we ensure, or at least increase, the interpretability of the machine?  

One approach, that is gaining traction, to solve this value alignment problem is called reinforcement learning (RL) \citep{soares2015value}. Reinforcement learning is based on the idea that an AI is embedded in an environment. The AI can change the state of the environment through its actions and in turn the AI receives a reward signal from the environment. The RL agent chooses its actions, and thus navigates through the environment, via maximizing the expected reward. If the reward signal is correctly specified, then the RL agent chooses the right action and can in this sense behave in a morally desirable way.

The problem of value learning, however, is further complicated given that an AGI is actively learning from its environment. That means that an AGI is already operational, but still changes its behaviour. There are two reasons why this seems desirable. First, there is good reason to believe that AGI learns faster the more data it gets. Scientists can only teach the AGI so much in their lab; once these products are sold, they can complete individual tasks and update their learning process via a cloud. Such a process would increase the learning rate manifold. Second, allowing the AGI to update its behaviour enables for personalisation. This is desirable for the manufacturing company as humans have individual preferences that deviate from the average preferences. For example, given that I am a coffee lover, I could teach my personal assistant AGI the specific way I want it to brew my coffee with the AeroPress.  

%There are two problems with this approach. First, ensuring that the agent chooses the right action depends on the reward signals that the programmer has to specify for the different environmental states. Even slight variations may cause the RL agent to choose a different, and thus morally less desirable or even impermissible, action. That is to say that RL alone is not sufficient to ensure that the agent \emph{reliably} behaves morally. Second, since the behaviour of the agent depends on the correct rewards, RL as an ethical framework implicitly assumes that the programmer is aware what is the correct action to take in a certain state of the world. This does not have to be the case. Ethics is a complicated subject, and we cannot expect computer scientists to be as proficient at it as ethicists. If we make use of RL alone to solve ethical problems, we need a very tight collaboration of computer scientists and ethicists, which might not be realistic.\todo{Change}

In this paper we propose the introduction of deontic norms into the reinforcement learning framework (DRL). These put a restriction on which actions are available to be performed by the agent in different states of the environment. When we choose appropriate norms we increase the reliability of the agent behaving in morally permissible ways. We anticipate the problem of conflicting norms and solve it by introducing a preference ranking over the norms. It is our belief that DRL is a fruitful approach for ethical decision making and propose a (non-exhaustive) list of further research.

%In this paper I propose combining RL and deontic logic into a single framework (DLRL). Deontic logic allows the assignment of deontic operators to action-state pairs. For example, we can define that it is \emph{obligatory} that a certain action is done in a certain state of the world. This ensures that the RL agent has to take that action in that state of the world, regardless of the reward signals. Furthermore, the assignment of deontic operators to action-state pairs can be done independently by ethicists. Hence, a less intense cooperation between ethicists and computer scientists is necessary.

In the second section we explicate what RL is and how it can be used as a framework for ethical decision making. In the third section we present DRL and illustrate the conditions that need to be satisfied so that DRL works properly. The fourth section gives a non-exhaustive overview of potential future research. 
